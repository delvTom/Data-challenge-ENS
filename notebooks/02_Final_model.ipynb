{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1e343e",
   "metadata": {},
   "source": [
    "# ðŸ§  Data Challenge 2025 â€“ Gas Detection\n",
    "\n",
    "This notebook presents the solution developed for the **Data Challenge 2025 (ENS / Bertin Technologies)**.  \n",
    "The objective of the challenge is to **predict the alarm level of 23 gases** based on multivariate sensor measurements.\n",
    "\n",
    "The final model relies on:\n",
    "- **Extra Trees Regressors** trained on several variants (with and without *row-wise* features),  \n",
    "- an **optimized blending** of predictions (both global and per-target),  \n",
    "- a **linear calibration** and **shrinkage toward the mean** to stabilize outputs.\n",
    "\n",
    "This notebook is designed to:\n",
    "- preprocess the data,  \n",
    "- train the models,  \n",
    "- and **automatically generate the final submission** in the format required by the challenge platform.\n",
    "\n",
    "Performance is evaluated using the **Weighted RMSE**, a weighted root mean squared error metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e79687",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Imports & Data Loading  \n",
    "Read the files `x_train.csv`, `y_train.csv`, and `x_test.csv` from the `../DATA` directory.  \n",
    "Display basic dimensions for a quick sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data import\n",
    "DATA_DIR = Path(\"../DATA\").resolve()\n",
    "assert DATA_DIR.exists(), f\"Le dossier DATA est introuvable: {DATA_DIR}\"\n",
    "\n",
    "X_train = pd.read_csv(DATA_DIR / \"x_train.csv\")\n",
    "y_train = pd.read_csv(DATA_DIR / \"y_train.csv\")\n",
    "X_test  = pd.read_csv(DATA_DIR / \"x_test.csv\")\n",
    "\n",
    "print(f\"âœ… DonnÃ©es chargÃ©es :\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfefd9d",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. Utility Functions  \n",
    "Reusable functions used throughout the pipeline: custom metric (Weighted RMSE), stratified folds,  \n",
    "row-wise cleaning and feature generation, linear calibration, and shrinkage toward the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59cbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weighted_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float: # Challenge mÃ©tric\n",
    "    y_pred = np.clip(y_pred, 0.0, 1.0)\n",
    "    w = np.where(y_true >= 0.5, 1.2, 1.0)\n",
    "    mse_per_sample = np.mean(w * (y_pred - y_true)**2, axis=1)\n",
    "    return float(np.sqrt(np.mean(mse_per_sample)))\n",
    "\n",
    "\n",
    "def make_stratified_folds(y: pd.DataFrame, n_splits=5, random_state=42): # Stratified folds based on target mean (for stable out-of-fold performance)\n",
    "    y_mean = y.mean(axis=1).to_numpy()\n",
    "    ranks = pd.Series(y_mean).rank(method=\"average\", pct=True).to_numpy()\n",
    "    nbins = max(2, min(10, len(y)//n_splits))\n",
    "    bins = np.floor(ranks * nbins).astype(int)\n",
    "    bins[bins == nbins] = nbins - 1\n",
    "    if len(np.unique(bins)) < 2:\n",
    "        print(\"âš ï¸ Stratification impossible (bins uniques) â†’ fallback KFold.\")\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        return list(kf.split(np.zeros(len(y))))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    return list(skf.split(np.zeros(len(y)), bins))\n",
    "\n",
    "\n",
    "# Clip by quantiles (fitted on train, applied to test) to reduce the influence of outliers\n",
    "# Drop 'Humidity' feature (too noisy according to EDA)\n",
    "def clean_features(X_train, X_test, lower_q=0.01, upper_q=0.99, drop_humidity=True):\n",
    "    Xtr = X_train.copy(); Xte = X_test.copy()\n",
    "    for col in Xtr.columns:\n",
    "        if col != \"ID\" and pd.api.types.is_numeric_dtype(Xtr[col]):\n",
    "            low = Xtr[col].quantile(lower_q)\n",
    "            high = Xtr[col].quantile(upper_q)\n",
    "            Xtr[col] = Xtr[col].clip(low, high)\n",
    "            Xte[col] = Xte[col].clip(low, high)\n",
    "    if drop_humidity and \"Humidity\" in Xtr.columns:\n",
    "        Xtr = Xtr.drop(columns=[\"Humidity\"]) ; Xte = Xte.drop(columns=[\"Humidity\"]) \n",
    "    return Xtr, Xte\n",
    "\n",
    "\n",
    "# Add robust \"row-level\" features (means/percentiles/spreads) to capture global sensor structure\n",
    "def add_rowwise_features(X: pd.DataFrame,\n",
    "                         exclude_cols=(\"ID\",),\n",
    "                         prefix=\"rw_\") -> pd.DataFrame:\n",
    "    Xo = X.copy()\n",
    "    num_cols = [c for c in Xo.columns if c not in exclude_cols and pd.api.types.is_numeric_dtype(Xo[c])]\n",
    "    if len(num_cols) == 0:\n",
    "        return Xo\n",
    "    V = Xo[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    def row_nanpercentile(a, p):\n",
    "        return np.nanpercentile(a, p, axis=1)\n",
    "\n",
    "    mean   = np.nanmean(V, axis=1)\n",
    "    median = np.nanmedian(V, axis=1)\n",
    "    std    = np.nanstd(V, axis=1)\n",
    "    vmin   = np.nanmin(V, axis=1)\n",
    "    vmax   = np.nanmax(V, axis=1)\n",
    "    rng    = vmax - vmin\n",
    "\n",
    "    q25 = row_nanpercentile(V, 25)\n",
    "    q75 = row_nanpercentile(V, 75)\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    abs_dev = np.abs(V - median[:, None])\n",
    "    mad = np.nanmedian(abs_dev, axis=1)\n",
    "\n",
    "    l1 = np.nansum(np.abs(V), axis=1)\n",
    "    l2 = np.sqrt(np.nansum(V * V, axis=1))\n",
    "\n",
    "    n_nan   = np.isnan(V).sum(axis=1).astype(float)\n",
    "    nan_rat = n_nan / V.shape[1]\n",
    "\n",
    "# Robust row-wise aggregates (nan-safe): useful when sensor distributions differ\n",
    "    R = pd.DataFrame({\n",
    "        f\"{prefix}mean\": mean,\n",
    "        f\"{prefix}median\": median,\n",
    "        f\"{prefix}std\": std,\n",
    "        f\"{prefix}min\": vmin,\n",
    "        f\"{prefix}max\": vmax,\n",
    "        f\"{prefix}range\": rng,\n",
    "        f\"{prefix}q25\": q25,\n",
    "        f\"{prefix}q75\": q75,\n",
    "        f\"{prefix}iqr\": iqr,\n",
    "        f\"{prefix}mad\": mad,\n",
    "        f\"{prefix}l1\": l1,\n",
    "        f\"{prefix}l2\": l2,\n",
    "        f\"{prefix}n_nan\": n_nan,\n",
    "        f\"{prefix}nan_ratio\": nan_rat,\n",
    "    }, index=Xo.index)\n",
    "\n",
    "    return pd.concat([Xo, R], axis=1)\n",
    "\n",
    "\n",
    "# Independent linear calibration per target: corrects scale bias between OOF and y_true\n",
    "# If variance is near zero â†’ fallback to target mean (avoids unstable solutions)\n",
    "def calibrate_predictions_linear(y_true_df, oof_pred_df, test_pred_df, eps=1e-9):\n",
    "    y_true = y_true_df.values\n",
    "    y_oof  = oof_pred_df.values\n",
    "    y_te   = test_pred_df.values.copy()\n",
    "    for j in range(y_true.shape[1]):\n",
    "        x = y_oof[:, [j]]\n",
    "        if np.std(x) < eps:\n",
    "            a, b = 0.0, float(y_true[:, j].mean())\n",
    "            y_oof[:, j] = a * x.ravel() + b\n",
    "            y_te[:,  j] = a * y_te[:,  j] + b\n",
    "        else:\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(x, y_true[:, j])\n",
    "            y_oof[:, j] = lr.predict(x)\n",
    "            y_te[:,  j] = lr.predict(y_te[:, [j]])\n",
    "    y_oof = np.clip(y_oof, 0.0, 1.0)\n",
    "    y_te  = np.clip(y_te,  0.0, 1.0)\n",
    "    return pd.DataFrame(y_oof, columns=y_true_df.columns), pd.DataFrame(y_te, columns=test_pred_df.columns)\n",
    "\n",
    "# Global shrinkage toward the training mean\n",
    "\n",
    "\n",
    "def shrink_to_mean(test_pred_df, y_train_df, alpha=0.95):\n",
    "    mu = y_train_df.mean(axis=0).values\n",
    "    Y  = test_pred_df.values\n",
    "    Ys = alpha * Y + (1 - alpha) * mu\n",
    "    return pd.DataFrame(np.clip(Ys, 0.0, 1.0), columns=test_pred_df.columns)\n",
    "\n",
    "# Target-wise shrinkage with distinct alpha_j values (allows adjusting shrinkage strength per column)\n",
    "\n",
    "def shrink_to_mean_per_target(pred_df: pd.DataFrame, y_train_df: pd.DataFrame, alphas: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"alphas: array shape (n_targets,) avec alpha_j pour chaque cible.\"\"\"\n",
    "    mu = y_train_df.mean(axis=0).to_numpy()          # (T,)\n",
    "    Y  = pred_df.to_numpy()                           # (N,T)\n",
    "    A  = np.asarray(alphas).reshape(1, -1)            # (1,T)\n",
    "    Ys = A * Y + (1.0 - A) * mu.reshape(1, -1)\n",
    "    return pd.DataFrame(np.clip(Ys, 0.0, 1.0), columns=pred_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9413fed",
   "metadata": {},
   "source": [
    "## ðŸŒ² 3. ExtraTrees Variants  \n",
    "Definition of a small family of ExtraTrees models (with and without *row-wise* features),  \n",
    "using different random seeds and settings (depth, bootstrap).  \n",
    "Default parameters are centralized, and each variant only specifies its overrides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration d'une variante ExtraTrees (nom, usage des features row-wise, overrides de params sklearn)\n",
    "@dataclass\n",
    "class ETConfig:\n",
    "    name: str\n",
    "    use_rw: bool\n",
    "    params: Dict\n",
    "\n",
    "\n",
    "def make_et(**kw) -> ExtraTreesRegressor:\n",
    "    return ExtraTreesRegressor(\n",
    "        n_estimators=kw.get(\"n_estimators\", 500),\n",
    "        max_features=kw.get(\"max_features\", 0.7),\n",
    "        min_samples_split=kw.get(\"min_samples_split\", 8),\n",
    "        min_samples_leaf=kw.get(\"min_samples_leaf\", 2),\n",
    "        max_depth=kw.get(\"max_depth\", None),\n",
    "        bootstrap=kw.get(\"bootstrap\", False),\n",
    "        max_samples=kw.get(\"max_samples\", None),\n",
    "        n_jobs=-1,\n",
    "        random_state=kw.get(\"random_state\", 42),\n",
    "    )\n",
    "\n",
    "VARIANTS: List[ETConfig] = [\n",
    "    ETConfig(\"ET-Base\", use_rw=False, params=dict(random_state=42)),\n",
    "    ETConfig(\"ET-RW\",   use_rw=True,  params=dict(random_state=42)),\n",
    "    ETConfig(\"ET-DepthCap\",   use_rw=False, params=dict(random_state=42, max_depth=24, max_features=0.8, min_samples_leaf=1)),\n",
    "    ETConfig(\"ET-Bootstrap\",    use_rw=True,  params=dict(random_state=42, bootstrap=True, max_samples=0.8, max_features=0.6, min_samples_leaf=3)),\n",
    "    ETConfig(\"ET-Seed13\",     use_rw=False, params=dict(random_state=13)),\n",
    "    ETConfig(\"ET-Seed71\",       use_rw=True,  params=dict(random_state=71)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631822c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_et_variant(\n",
    "    cfg: ETConfig,\n",
    "    X_base_tr: pd.DataFrame,\n",
    "    X_base_te: pd.DataFrame,\n",
    "    X_rw_tr: pd.DataFrame,\n",
    "    X_rw_te: pd.DataFrame,\n",
    "    y_fit: pd.DataFrame,\n",
    "    n_splits: int = 5,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \n",
    "   # Stratified folds based on target mean\n",
    "    folds = make_stratified_folds(y_fit, n_splits=n_splits, random_state=seed)\n",
    "\n",
    "    # Feature selection based on variant (base vs row-wise); drop 'ID' if present\n",
    "    Xtr = (X_rw_tr if cfg.use_rw else X_base_tr).drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "    Xte = (X_rw_te if cfg.use_rw else X_base_te).drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "    oof = np.zeros((len(Xtr), y_fit.shape[1]), dtype=float)\n",
    "    te  = np.zeros((len(Xte), y_fit.shape[1]), dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "        print(f\"ðŸš€ {cfg.name} â€” Fold {i}/{n_splits}\")\n",
    "        X_tr, X_va = Xtr.iloc[tr_idx], Xtr.iloc[va_idx]\n",
    "        y_tr, y_va = y_fit.iloc[tr_idx], y_fit.iloc[va_idx]\n",
    "\n",
    "        model = MultiOutputRegressor(make_et(**cfg.params), n_jobs=1)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        oof[va_idx] = model.predict(X_va)\n",
    "        te += model.predict(Xte) / n_splits\n",
    "\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # OOF score using the challenge's evaluation metric\n",
    "\n",
    "    score = weighted_rmse(y_fit.values, oof)\n",
    "    print(f\"ðŸŽ¯ {cfg.name} â€” OOF Weighted RMSE: {score:.6f} | â± {elapsed:.1f}s\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    return oof, te, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e605a",
   "metadata": {},
   "source": [
    "## ðŸ§© 4. Model Blending  \n",
    "Optimization of weights on the Dirichlet simplex to combine out-of-fold (OOF) predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample weights (n_samples, k) on the simplex using a Dirichlet distribution\n",
    "def _dirichlet_weights(k: int, n_samples: int = 4000, temperature: float = 1.0, rng: np.random.Generator | None = None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    alpha = np.ones(k) * temperature\n",
    "    return rng.dirichlet(alpha, size=n_samples)  # (n_samples, k)\n",
    "\n",
    "\n",
    "def optimize_global_weights(oofs: List[np.ndarray], y_true: np.ndarray, n_samples: int = 6000) -> np.ndarray:\n",
    "    k = len(oofs)\n",
    "    W = _dirichlet_weights(k, n_samples=n_samples)\n",
    "    oofs_stack = np.stack(oofs, axis=-1)  # (n_samples, n_targets, k)\n",
    "    best_w = None\n",
    "    best_score = float(\"inf\")\n",
    "    for w in W:\n",
    "        y_blend = np.tensordot(oofs_stack, w, axes=([2],[0]))  # (n_samples, n_targets)\n",
    "        score = weighted_rmse(y_true, y_blend)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_w = w\n",
    "    print(f\"âœ… Best global blend (OOF): {best_score:.6f} | weights={np.round(best_w,3)}\")\n",
    "    return best_w\n",
    "\n",
    "\n",
    "def optimize_per_target_weights(oofs: List[np.ndarray], y_true: np.ndarray, n_samples: int = 3000) -> np.ndarray:\n",
    "    k = len(oofs)\n",
    "    T = y_true.shape[1]\n",
    "    oofs_stack = np.stack(oofs, axis=-1)  # (n_samples, n_targets, k)\n",
    "    W_out = np.zeros((T, k), dtype=float)\n",
    "    for j in range(T):\n",
    "        W = _dirichlet_weights(k, n_samples=n_samples)\n",
    "        best_w = None\n",
    "        best = float(\"inf\")\n",
    "        yj_true = y_true[:, j]\n",
    "        oofs_j = oofs_stack[:, j, :]  # (n_samples, k)\n",
    "        for w in W:\n",
    "            yj_pred = oofs_j @ w\n",
    "            yj_pred = np.clip(yj_pred, 0, 1)\n",
    "            wgt = np.where(yj_true >= 0.5, 1.2, 1.0)\n",
    "            mse = np.mean(wgt * (yj_pred - yj_true) ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            if rmse < best:\n",
    "                best = rmse\n",
    "                best_w = w\n",
    "        W_out[j] = best_w\n",
    "    return W_out\n",
    "\n",
    "\n",
    "def apply_global_weights(oofs: List[np.ndarray], tests: List[np.ndarray], w: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    oof_stack = np.stack(oofs, axis=-1)\n",
    "    test_stack = np.stack(tests, axis=-1)\n",
    "    oof_blend = np.tensordot(oof_stack, w, axes=([2],[0]))\n",
    "    test_blend = np.tensordot(test_stack, w, axes=([2],[0]))\n",
    "    return np.clip(oof_blend, 0, 1), np.clip(test_blend, 0, 1)\n",
    "\n",
    "\n",
    "def apply_per_target_weights(oofs: List[np.ndarray], tests: List[np.ndarray], Wt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    oof_stack = np.stack(oofs, axis=-1)\n",
    "    test_stack = np.stack(tests, axis=-1)\n",
    "    oof_blend = np.einsum('ntk,tk->nt', oof_stack, Wt)\n",
    "    test_blend = np.einsum('mtk,tk->mt', test_stack, Wt)\n",
    "    return np.clip(oof_blend, 0, 1), np.clip(test_blend, 0, 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5ef8c",
   "metadata": {},
   "source": [
    "## ðŸ§ª 5. Pipeline: diversity + blending + calibration + shrinkage  \n",
    "All model variants are trained, blend weights are optimized on OOF predictions (globally and per target),  \n",
    "then a linear calibration and shrinkage toward the training mean are applied.  \n",
    "The pipeline returns the final OOF/Test predictions along with metadata (weights, alpha, scores).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_step1_diversity_blend(cv_splits: int = 5, seed: int = 42, do_per_target_weights: bool = True,\n",
    "                              shrink_grid=(0.92,0.95,0.97,0.99)):\n",
    "    \"\"\"EntraÃ®ne les variantes ET, optimise le blend sur OOF, applique calibration + shrink,\n",
    "                                retourne les rÃ©sultats.\"\"\"\n",
    "\n",
    "\n",
    "   # 1) Cleaning & feature engineering\n",
    "    X_base_tr, X_base_te = clean_features(X_train, X_test, drop_humidity=True)\n",
    "    X_rw_tr  = add_rowwise_features(X_base_tr, exclude_cols=(\"ID\",))\n",
    "    X_rw_te  = add_rowwise_features(X_base_te, exclude_cols=(\"ID\",))\n",
    "\n",
    "   # 2) Targets: remove 'ID' and 'c15'\n",
    "    y_fit = y_train.drop(columns=[\"ID\", \"c15\"], errors=\"ignore\").copy()\n",
    "    y_fit = y_fit.loc[X_base_tr.index]\n",
    "\n",
    "    # 3) Train model variants\n",
    "    oofs: List[np.ndarray] = []\n",
    "    tests: List[np.ndarray] = []\n",
    "    scores: List[Tuple[str, float]] = []\n",
    "    for cfg in VARIANTS:\n",
    "        oof, te, sc = run_et_variant(cfg, X_base_tr, X_base_te, X_rw_tr, X_rw_te, y_fit, n_splits=cv_splits, seed=seed)\n",
    "        oofs.append(oof); tests.append(te); scores.append((cfg.name, sc))\n",
    "\n",
    "    print(\"=== RÃ©sumÃ© variantes (OOF) ===\")\n",
    "    for name, sc in scores:\n",
    "        print(f\"{name:<24} : {sc:.6f}\")\n",
    "\n",
    "    # 4) Blends\n",
    "    y_true = y_fit.values\n",
    "    w_glob = optimize_global_weights(oofs, y_true, n_samples=6000)\n",
    "    oof_g, test_g = apply_global_weights(oofs, tests, w_glob)\n",
    "    score_g = weighted_rmse(y_true, oof_g)\n",
    "    print(f\"ðŸ Blend GLOBAL â€” OOF: {score_g:.6f}\")\n",
    "\n",
    "    blends = {\"global\": (oof_g, test_g, w_glob, score_g)}\n",
    "\n",
    "    if do_per_target_weights:\n",
    "        Wt = optimize_per_target_weights(oofs, y_true, n_samples=3000)\n",
    "        oof_t, test_t = apply_per_target_weights(oofs, tests, Wt)\n",
    "        score_t = weighted_rmse(y_true, oof_t)\n",
    "        print(f\"ðŸ Blend PAR CIBLE â€” OOF: {score_t:.6f}\")\n",
    "        blends[\"per_target\"] = (oof_t, test_t, Wt, score_t)\n",
    "\n",
    "    # 5) Calibration + Shrink\n",
    "    results = {}\n",
    "    for k, (oof_b, test_b, W, sc_b) in blends.items():\n",
    "        oof_df  = pd.DataFrame(oof_b,  columns=y_fit.columns)\n",
    "        test_df = pd.DataFrame(test_b, columns=y_fit.columns)\n",
    "        oof_cal, test_cal = calibrate_predictions_linear(y_fit, oof_df, test_df)\n",
    "\n",
    "        best_alpha, best_score = None, float(\"inf\")\n",
    "        for a in shrink_grid:\n",
    "            tmp = shrink_to_mean(oof_cal, y_fit, alpha=a)\n",
    "            s = weighted_rmse(y_true, tmp.values)\n",
    "            if s < best_score:\n",
    "                best_alpha, best_score = a, s\n",
    "        print(f\"ðŸ”§ Blend={k}: meilleur alpha={best_alpha} â†’ OOF aprÃ¨s calib+shrink: {best_score:.6f}\")\n",
    "\n",
    "        oof_final  = shrink_to_mean(oof_cal,  y_fit, alpha=best_alpha).values\n",
    "        test_final = shrink_to_mean(test_cal, y_fit, alpha=best_alpha).values\n",
    "        oof_final  = np.clip(oof_final,  0, 1)\n",
    "        test_final = np.clip(test_final, 0, 1)\n",
    "\n",
    "        results[k] = {\n",
    "            \"weights\": W,\n",
    "            \"oof_pre_calib\": sc_b,\n",
    "            \"oof_post\": best_score,\n",
    "            \"alpha\": best_alpha,\n",
    "            \"oof_preds\": oof_final,\n",
    "            \"test_preds\": test_final,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # 6) Final selection\n",
    "    best_key = min(results.keys(), key=lambda k: results[k][\"oof_post\"])\n",
    "    print(f\"âœ… SÃ©lection finale: {best_key} â€” OOF={results[best_key]['oof_post']:.6f} (alpha={results[best_key]['alpha']})\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"variant_scores\": scores,\n",
    "        \"blends\": results,\n",
    "        \"best\": best_key,\n",
    "        \"y_columns\": y_fit.columns.tolist(),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43386cba",
   "metadata": {},
   "source": [
    "## ðŸ 6. Pipeline Execution & Submission Generation  \n",
    "The full pipeline is executed (variants â†’ blending â†’ calibration â†’ shrinkage),  \n",
    "the best blend is selected, and the final submission file is generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09649cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Run the pipeline (CV=3 for demo; use 5 for final run)\n",
    "out = run_step1_diversity_blend(cv_splits=3, seed=42, do_per_target_weights=True)\n",
    "\n",
    "# Retrieve the key of the best blend and its test predictions\n",
    "best_key = out[\"best\"]\n",
    "y_pred_blend = out[\"blends\"][best_key][\"test_preds\"]  # (n_test, n_targets_sans_c15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Target columns: exclude 'ID' and 'c15' from model fitting\n",
    "all_targets = [c for c in y_train.columns if c != \"ID\"]\n",
    "cols_fit = [c for c in all_targets if c != \"c15\"]\n",
    "\n",
    "# Create the prediction DataFrame (clip values to stay within [0,1])\n",
    "pred_df = pd.DataFrame(np.clip(y_pred_blend, 0, 1), columns=cols_fit)\n",
    "\n",
    "# RÃ©insÃ¨re la cible manquante 'c15'\n",
    "pred_df.insert(all_targets.index(\"c15\"), \"c15\", 0.0)\n",
    "\n",
    "# Assemble la soumission finale avec ID et ordre exact des colonnes\n",
    "submission = pd.concat([X_test[\"ID\"], pred_df[all_targets]], axis=1)\n",
    "submission.to_csv(\"submission_ET_BLEND_v1.csv\", index=False)\n",
    "\n",
    "\n",
    "# Quick diagnostics\n",
    "vals = submission.drop(columns=[\"ID\"]).to_numpy()\n",
    "assert np.isfinite(vals).all(), \"NaN/Inf dÃ©tectÃ©s dans la soumission\"\n",
    "assert (vals >= -1e-9).all() and (vals <= 1+1e-9).all(), \"Valeurs hors [0,1]\"\n",
    "assert submission.index.equals(X_test.index)\n",
    "print(\"âœ… Submission prÃªte :\", submission.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
