{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a20ddfb8",
   "metadata": {},
   "source": [
    "# üß† Data Challenge 2025 ‚Äì D√©tection de gaz\n",
    "\n",
    "Ce notebook pr√©sente la solution d√©velopp√©e pour le **Data Challenge 2025 (ENS / Bertin Technologies)**.  \n",
    "L‚Äôobjectif du challenge est de **pr√©dire le niveau d'alarme de 23 gaz** √† partir de mesures capteurs multivari√©es.  \n",
    "\n",
    "Le mod√®le final repose sur :\n",
    "- des **Extra Trees Regressors** entra√Æn√©s sur diff√©rentes variantes (avec et sans features \"row-wise\"),  \n",
    "- un **blending optimis√©** des pr√©dictions (global et par cible),  \n",
    "- une **calibration lin√©aire** et un **shrinkage vers la moyenne** pour stabiliser les sorties.  \n",
    "\n",
    "Le notebook permet de :\n",
    "- pr√©traiter les donn√©es,  \n",
    "- entra√Æner les mod√®les,  \n",
    "- et **g√©n√©rer automatiquement la soumission finale** au format attendu par la plateforme du challenge.\n",
    "\n",
    "La performance est √©valu√©e avec la **Weighted RMSE**, une racine d‚Äôerreur quadratique moyenne\n",
    "pond√©r√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4f60a",
   "metadata": {},
   "source": [
    "## üì¶ 1. Imports & Chargement des donn√©es\n",
    "Lecture des fichiers `x_train.csv`, `y_train.csv` et `x_test.csv` √† partir du dossier `../DATA`.  \n",
    "On affiche les dimensions pour v√©rification rapide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31aca213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es charg√©es :\n",
      "X_train: (202933, 14), y_train: (202933, 24), X_test: (134673, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Import des donn√©es\n",
    "DATA_DIR = Path(\"../DATA\").resolve()\n",
    "assert DATA_DIR.exists(), f\"Le dossier DATA est introuvable: {DATA_DIR}\"\n",
    "\n",
    "X_train = pd.read_csv(DATA_DIR / \"x_train.csv\")\n",
    "y_train = pd.read_csv(DATA_DIR / \"y_train.csv\")\n",
    "X_test  = pd.read_csv(DATA_DIR / \"x_test.csv\")\n",
    "\n",
    "print(f\"‚úÖ Donn√©es charg√©es :\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f2c83",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. Fonctions utilitaires\n",
    "Fonctions r√©utilis√©es dans tout le pipeline : m√©trique (weighted RMSE), folds stratifi√©s,\n",
    "nettoyage/featurisation ligne, calibration lin√©aire et shrinkage vers la moyenne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21982fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weighted_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float: # M√©trique du challenge \n",
    "    y_pred = np.clip(y_pred, 0.0, 1.0)\n",
    "    w = np.where(y_true >= 0.5, 1.2, 1.0)\n",
    "    mse_per_sample = np.mean(w * (y_pred - y_true)**2, axis=1)\n",
    "    return float(np.sqrt(np.mean(mse_per_sample)))\n",
    "\n",
    "\n",
    "def make_stratified_folds(y: pd.DataFrame, n_splits=5, random_state=42): # Folds stratifi√©s sur la moyenne des cibles (stabilit√© OOF).\n",
    "    y_mean = y.mean(axis=1).to_numpy()\n",
    "    ranks = pd.Series(y_mean).rank(method=\"average\", pct=True).to_numpy()\n",
    "    nbins = max(2, min(10, len(y)//n_splits))\n",
    "    bins = np.floor(ranks * nbins).astype(int)\n",
    "    bins[bins == nbins] = nbins - 1\n",
    "    if len(np.unique(bins)) < 2:\n",
    "        print(\"‚ö†Ô∏è Stratification impossible (bins uniques) ‚Üí fallback KFold.\")\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        return list(kf.split(np.zeros(len(y))))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    return list(skf.split(np.zeros(len(y)), bins))\n",
    "\n",
    "\n",
    "# Clip par quantiles (fit sur train, appliqu√© √† test) pour limiter l'influence des outliers.\n",
    "# On retire 'Humidity' (trop bruit√©e selon EDA).\n",
    "def clean_features(X_train, X_test, lower_q=0.01, upper_q=0.99, drop_humidity=True):\n",
    "    Xtr = X_train.copy(); Xte = X_test.copy()\n",
    "    for col in Xtr.columns:\n",
    "        if col != \"ID\" and pd.api.types.is_numeric_dtype(Xtr[col]):\n",
    "            low = Xtr[col].quantile(lower_q)\n",
    "            high = Xtr[col].quantile(upper_q)\n",
    "            Xtr[col] = Xtr[col].clip(low, high)\n",
    "            Xte[col] = Xte[col].clip(low, high)\n",
    "    if drop_humidity and \"Humidity\" in Xtr.columns:\n",
    "        Xtr = Xtr.drop(columns=[\"Humidity\"]) ; Xte = Xte.drop(columns=[\"Humidity\"]) \n",
    "    return Xtr, Xte\n",
    "\n",
    "\n",
    "# Ajoute des features \"ligne\" robustes (moyennes/percentiles/√©carts) pour capturer structure globale de capteurs.\n",
    "def add_rowwise_features(X: pd.DataFrame,\n",
    "                         exclude_cols=(\"ID\",),\n",
    "                         prefix=\"rw_\") -> pd.DataFrame:\n",
    "    Xo = X.copy()\n",
    "    num_cols = [c for c in Xo.columns if c not in exclude_cols and pd.api.types.is_numeric_dtype(Xo[c])]\n",
    "    if len(num_cols) == 0:\n",
    "        return Xo\n",
    "    V = Xo[num_cols].to_numpy(dtype=float)\n",
    "\n",
    "    def row_nanpercentile(a, p):\n",
    "        return np.nanpercentile(a, p, axis=1)\n",
    "\n",
    "    mean   = np.nanmean(V, axis=1)\n",
    "    median = np.nanmedian(V, axis=1)\n",
    "    std    = np.nanstd(V, axis=1)\n",
    "    vmin   = np.nanmin(V, axis=1)\n",
    "    vmax   = np.nanmax(V, axis=1)\n",
    "    rng    = vmax - vmin\n",
    "\n",
    "    q25 = row_nanpercentile(V, 25)\n",
    "    q75 = row_nanpercentile(V, 75)\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    abs_dev = np.abs(V - median[:, None])\n",
    "    mad = np.nanmedian(abs_dev, axis=1)\n",
    "\n",
    "    l1 = np.nansum(np.abs(V), axis=1)\n",
    "    l2 = np.sqrt(np.nansum(V * V, axis=1))\n",
    "\n",
    "    n_nan   = np.isnan(V).sum(axis=1).astype(float)\n",
    "    nan_rat = n_nan / V.shape[1]\n",
    "\n",
    "# Agr√©gats robustes par ligne (nan-safe) : utiles si les distributions par capteur diff√®rent\n",
    "    R = pd.DataFrame({\n",
    "        f\"{prefix}mean\": mean,\n",
    "        f\"{prefix}median\": median,\n",
    "        f\"{prefix}std\": std,\n",
    "        f\"{prefix}min\": vmin,\n",
    "        f\"{prefix}max\": vmax,\n",
    "        f\"{prefix}range\": rng,\n",
    "        f\"{prefix}q25\": q25,\n",
    "        f\"{prefix}q75\": q75,\n",
    "        f\"{prefix}iqr\": iqr,\n",
    "        f\"{prefix}mad\": mad,\n",
    "        f\"{prefix}l1\": l1,\n",
    "        f\"{prefix}l2\": l2,\n",
    "        f\"{prefix}n_nan\": n_nan,\n",
    "        f\"{prefix}nan_ratio\": nan_rat,\n",
    "    }, index=Xo.index)\n",
    "\n",
    "    return pd.concat([Xo, R], axis=1)\n",
    "\n",
    "\n",
    "# Calibration lin√©aire ind√©pendante par cible : corrige biais d'√©chelle entre OOF et y_true.\n",
    "# Si variance quasi nulle ‚Üí on remet √† la moyenne de la cible (√©vite les solutions instables).\n",
    "def calibrate_predictions_linear(y_true_df, oof_pred_df, test_pred_df, eps=1e-9):\n",
    "    y_true = y_true_df.values\n",
    "    y_oof  = oof_pred_df.values\n",
    "    y_te   = test_pred_df.values.copy()\n",
    "    for j in range(y_true.shape[1]):\n",
    "        x = y_oof[:, [j]]\n",
    "        if np.std(x) < eps:\n",
    "            a, b = 0.0, float(y_true[:, j].mean())\n",
    "            y_oof[:, j] = a * x.ravel() + b\n",
    "            y_te[:,  j] = a * y_te[:,  j] + b\n",
    "        else:\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(x, y_true[:, j])\n",
    "            y_oof[:, j] = lr.predict(x)\n",
    "            y_te[:,  j] = lr.predict(y_te[:, [j]])\n",
    "    y_oof = np.clip(y_oof, 0.0, 1.0)\n",
    "    y_te  = np.clip(y_te,  0.0, 1.0)\n",
    "    return pd.DataFrame(y_oof, columns=y_true_df.columns), pd.DataFrame(y_te, columns=test_pred_df.columns)\n",
    "\n",
    "# Shrinkage global vers la moyenne d'entra√Ænement.\n",
    "\n",
    "\n",
    "def shrink_to_mean(test_pred_df, y_train_df, alpha=0.95):\n",
    "    mu = y_train_df.mean(axis=0).values\n",
    "    Y  = test_pred_df.values\n",
    "    Ys = alpha * Y + (1 - alpha) * mu\n",
    "    return pd.DataFrame(np.clip(Ys, 0.0, 1.0), columns=test_pred_df.columns)\n",
    "\n",
    "# Shrinkage par cible avec alpha_j distincts (permet d'ajuster l'ampleur par colonne)\n",
    "\n",
    "def shrink_to_mean_per_target(pred_df: pd.DataFrame, y_train_df: pd.DataFrame, alphas: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"alphas: array shape (n_targets,) avec alpha_j pour chaque cible.\"\"\"\n",
    "    mu = y_train_df.mean(axis=0).to_numpy()          # (T,)\n",
    "    Y  = pred_df.to_numpy()                           # (N,T)\n",
    "    A  = np.asarray(alphas).reshape(1, -1)            # (1,T)\n",
    "    Ys = A * Y + (1.0 - A) * mu.reshape(1, -1)\n",
    "    return pd.DataFrame(np.clip(Ys, 0.0, 1.0), columns=pred_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928f4ba",
   "metadata": {},
   "source": [
    "## üå≤ 3. Variantes ExtraTrees\n",
    "D√©finition d'une petite famille de mod√®les ExtraTrees (avec et sans features \"row-wise\"),\n",
    "plusieurs seeds et r√©glages (profondeur, bootstrap). Les param√®tres par d√©faut sont\n",
    "centralis√©s et chaque variante ne pr√©cise que ses overrides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0df84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration d'une variante ExtraTrees (nom, usage des features row-wise, overrides de params sklearn)\n",
    "@dataclass\n",
    "class ETConfig:\n",
    "    name: str\n",
    "    use_rw: bool\n",
    "    params: Dict\n",
    "\n",
    "\n",
    "def make_et(**kw) -> ExtraTreesRegressor:\n",
    "    return ExtraTreesRegressor(\n",
    "        n_estimators=kw.get(\"n_estimators\", 500),\n",
    "        max_features=kw.get(\"max_features\", 0.7),\n",
    "        min_samples_split=kw.get(\"min_samples_split\", 8),\n",
    "        min_samples_leaf=kw.get(\"min_samples_leaf\", 2),\n",
    "        max_depth=kw.get(\"max_depth\", None),\n",
    "        bootstrap=kw.get(\"bootstrap\", False),\n",
    "        max_samples=kw.get(\"max_samples\", None),\n",
    "        n_jobs=-1,\n",
    "        random_state=kw.get(\"random_state\", 42),\n",
    "    )\n",
    "\n",
    "VARIANTS: List[ETConfig] = [\n",
    "    ETConfig(\"ET-Base\", use_rw=False, params=dict(random_state=42)),\n",
    "    ETConfig(\"ET-RW\",   use_rw=True,  params=dict(random_state=42)),\n",
    "    ETConfig(\"ET-DepthCap\",   use_rw=False, params=dict(random_state=42, max_depth=24, max_features=0.8, min_samples_leaf=1)),\n",
    "    ETConfig(\"ET-Bootstrap\",    use_rw=True,  params=dict(random_state=42, bootstrap=True, max_samples=0.8, max_features=0.6, min_samples_leaf=3)),\n",
    "    ETConfig(\"ET-Seed13\",     use_rw=False, params=dict(random_state=13)),\n",
    "    ETConfig(\"ET-Seed71\",       use_rw=True,  params=dict(random_state=71)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3a4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_et_variant(\n",
    "    cfg: ETConfig,\n",
    "    X_base_tr: pd.DataFrame,\n",
    "    X_base_te: pd.DataFrame,\n",
    "    X_rw_tr: pd.DataFrame,\n",
    "    X_rw_te: pd.DataFrame,\n",
    "    y_fit: pd.DataFrame,\n",
    "    n_splits: int = 5,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \n",
    "    # Folds stratifi√©s sur la moyenne des cibles\n",
    "    folds = make_stratified_folds(y_fit, n_splits=n_splits, random_state=seed)\n",
    "\n",
    "     # S√©lection des features selon la variante (base vs row-wise), on retire 'ID' si pr√©sent\n",
    "    Xtr = (X_rw_tr if cfg.use_rw else X_base_tr).drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "    Xte = (X_rw_te if cfg.use_rw else X_base_te).drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "    oof = np.zeros((len(Xtr), y_fit.shape[1]), dtype=float)\n",
    "    te  = np.zeros((len(Xte), y_fit.shape[1]), dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "        print(f\"üöÄ {cfg.name} ‚Äî Fold {i}/{n_splits}\")\n",
    "        X_tr, X_va = Xtr.iloc[tr_idx], Xtr.iloc[va_idx]\n",
    "        y_tr, y_va = y_fit.iloc[tr_idx], y_fit.iloc[va_idx]\n",
    "\n",
    "        model = MultiOutputRegressor(make_et(**cfg.params), n_jobs=1)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        oof[va_idx] = model.predict(X_va)\n",
    "        te += model.predict(Xte) / n_splits\n",
    "\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Score OOF avec la m√©tique du challenge \n",
    "\n",
    "    score = weighted_rmse(y_fit.values, oof)\n",
    "    print(f\"üéØ {cfg.name} ‚Äî OOF Weighted RMSE: {score:.6f} | ‚è± {elapsed:.1f}s\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    return oof, te, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f4a96",
   "metadata": {},
   "source": [
    "## üß© 4. Blend des mod√®les\n",
    "Optimisation des poids sur le simplexe Dirichlet pour combiner les OOF.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c1af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# √âchantillonne des poids (n_samples, k) sur le simplex via Dirichlet.\n",
    "def _dirichlet_weights(k: int, n_samples: int = 4000, temperature: float = 1.0, rng: np.random.Generator | None = None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    alpha = np.ones(k) * temperature\n",
    "    return rng.dirichlet(alpha, size=n_samples)  # (n_samples, k)\n",
    "\n",
    "\n",
    "def optimize_global_weights(oofs: List[np.ndarray], y_true: np.ndarray, n_samples: int = 6000) -> np.ndarray:\n",
    "    k = len(oofs)\n",
    "    W = _dirichlet_weights(k, n_samples=n_samples)\n",
    "    oofs_stack = np.stack(oofs, axis=-1)  # (n_samples, n_targets, k)\n",
    "    best_w = None\n",
    "    best_score = float(\"inf\")\n",
    "    for w in W:\n",
    "        y_blend = np.tensordot(oofs_stack, w, axes=([2],[0]))  # (n_samples, n_targets)\n",
    "        score = weighted_rmse(y_true, y_blend)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_w = w\n",
    "    print(f\"‚úÖ Best global blend (OOF): {best_score:.6f} | weights={np.round(best_w,3)}\")\n",
    "    return best_w\n",
    "\n",
    "\n",
    "def optimize_per_target_weights(oofs: List[np.ndarray], y_true: np.ndarray, n_samples: int = 3000) -> np.ndarray:\n",
    "    k = len(oofs)\n",
    "    T = y_true.shape[1]\n",
    "    oofs_stack = np.stack(oofs, axis=-1)  # (n_samples, n_targets, k)\n",
    "    W_out = np.zeros((T, k), dtype=float)\n",
    "    for j in range(T):\n",
    "        W = _dirichlet_weights(k, n_samples=n_samples)\n",
    "        best_w = None\n",
    "        best = float(\"inf\")\n",
    "        yj_true = y_true[:, j]\n",
    "        oofs_j = oofs_stack[:, j, :]  # (n_samples, k)\n",
    "        for w in W:\n",
    "            yj_pred = oofs_j @ w\n",
    "            yj_pred = np.clip(yj_pred, 0, 1)\n",
    "            wgt = np.where(yj_true >= 0.5, 1.2, 1.0)\n",
    "            mse = np.mean(wgt * (yj_pred - yj_true) ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            if rmse < best:\n",
    "                best = rmse\n",
    "                best_w = w\n",
    "        W_out[j] = best_w\n",
    "    return W_out\n",
    "\n",
    "\n",
    "def apply_global_weights(oofs: List[np.ndarray], tests: List[np.ndarray], w: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    oof_stack = np.stack(oofs, axis=-1)\n",
    "    test_stack = np.stack(tests, axis=-1)\n",
    "    oof_blend = np.tensordot(oof_stack, w, axes=([2],[0]))\n",
    "    test_blend = np.tensordot(test_stack, w, axes=([2],[0]))\n",
    "    return np.clip(oof_blend, 0, 1), np.clip(test_blend, 0, 1)\n",
    "\n",
    "\n",
    "def apply_per_target_weights(oofs: List[np.ndarray], tests: List[np.ndarray], Wt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    oof_stack = np.stack(oofs, axis=-1)\n",
    "    test_stack = np.stack(tests, axis=-1)\n",
    "    oof_blend = np.einsum('ntk,tk->nt', oof_stack, Wt)\n",
    "    test_blend = np.einsum('mtk,tk->mt', test_stack, Wt)\n",
    "    return np.clip(oof_blend, 0, 1), np.clip(test_blend, 0, 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab89c9d",
   "metadata": {},
   "source": [
    "## üß™ 5. Pipeline : diversit√© + blend + calibration + shrink\n",
    "On entra√Æne toutes les variantes, on optimise les poids de blend sur OOF (global et par cible),\n",
    "puis on applique une calibration lin√©aire et un shrinkage vers la moyenne. On retourne les\n",
    "pr√©dictions OOF/Test finales et les m√©tadonn√©es (poids, alpha, scores).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d097cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_step1_diversity_blend(cv_splits: int = 5, seed: int = 42, do_per_target_weights: bool = True,\n",
    "                              shrink_grid=(0.92,0.95,0.97,0.99)):\n",
    "    \"\"\"Entra√Æne les variantes ET, optimise le blend sur OOF, applique calibration + shrink,\n",
    "                                retourne les r√©sultats.\"\"\"\n",
    "\n",
    "\n",
    "    # 1) Nettoyage & features\n",
    "    X_base_tr, X_base_te = clean_features(X_train, X_test, drop_humidity=True)\n",
    "    X_rw_tr  = add_rowwise_features(X_base_tr, exclude_cols=(\"ID\",))\n",
    "    X_rw_te  = add_rowwise_features(X_base_te, exclude_cols=(\"ID\",))\n",
    "\n",
    "    # 2) Cibles : enl√®ve ID et c15\n",
    "    y_fit = y_train.drop(columns=[\"ID\", \"c15\"], errors=\"ignore\").copy()\n",
    "    y_fit = y_fit.loc[X_base_tr.index]\n",
    "\n",
    "    # 3) Entra√Æne variantes\n",
    "    oofs: List[np.ndarray] = []\n",
    "    tests: List[np.ndarray] = []\n",
    "    scores: List[Tuple[str, float]] = []\n",
    "    for cfg in VARIANTS:\n",
    "        oof, te, sc = run_et_variant(cfg, X_base_tr, X_base_te, X_rw_tr, X_rw_te, y_fit, n_splits=cv_splits, seed=seed)\n",
    "        oofs.append(oof); tests.append(te); scores.append((cfg.name, sc))\n",
    "\n",
    "    print(\"=== R√©sum√© variantes (OOF) ===\")\n",
    "    for name, sc in scores:\n",
    "        print(f\"{name:<24} : {sc:.6f}\")\n",
    "\n",
    "    # 4) Blends\n",
    "    y_true = y_fit.values\n",
    "    w_glob = optimize_global_weights(oofs, y_true, n_samples=6000)\n",
    "    oof_g, test_g = apply_global_weights(oofs, tests, w_glob)\n",
    "    score_g = weighted_rmse(y_true, oof_g)\n",
    "    print(f\"üèÅ Blend GLOBAL ‚Äî OOF: {score_g:.6f}\")\n",
    "\n",
    "    blends = {\"global\": (oof_g, test_g, w_glob, score_g)}\n",
    "\n",
    "    if do_per_target_weights:\n",
    "        Wt = optimize_per_target_weights(oofs, y_true, n_samples=3000)\n",
    "        oof_t, test_t = apply_per_target_weights(oofs, tests, Wt)\n",
    "        score_t = weighted_rmse(y_true, oof_t)\n",
    "        print(f\"üèÅ Blend PAR CIBLE ‚Äî OOF: {score_t:.6f}\")\n",
    "        blends[\"per_target\"] = (oof_t, test_t, Wt, score_t)\n",
    "\n",
    "    # 5) Calibration + Shrink\n",
    "    results = {}\n",
    "    for k, (oof_b, test_b, W, sc_b) in blends.items():\n",
    "        oof_df  = pd.DataFrame(oof_b,  columns=y_fit.columns)\n",
    "        test_df = pd.DataFrame(test_b, columns=y_fit.columns)\n",
    "        oof_cal, test_cal = calibrate_predictions_linear(y_fit, oof_df, test_df)\n",
    "\n",
    "        best_alpha, best_score = None, float(\"inf\")\n",
    "        for a in shrink_grid:\n",
    "            tmp = shrink_to_mean(oof_cal, y_fit, alpha=a)\n",
    "            s = weighted_rmse(y_true, tmp.values)\n",
    "            if s < best_score:\n",
    "                best_alpha, best_score = a, s\n",
    "        print(f\"üîß Blend={k}: meilleur alpha={best_alpha} ‚Üí OOF apr√®s calib+shrink: {best_score:.6f}\")\n",
    "\n",
    "        oof_final  = shrink_to_mean(oof_cal,  y_fit, alpha=best_alpha).values\n",
    "        test_final = shrink_to_mean(test_cal, y_fit, alpha=best_alpha).values\n",
    "        oof_final  = np.clip(oof_final,  0, 1)\n",
    "        test_final = np.clip(test_final, 0, 1)\n",
    "\n",
    "        results[k] = {\n",
    "            \"weights\": W,\n",
    "            \"oof_pre_calib\": sc_b,\n",
    "            \"oof_post\": best_score,\n",
    "            \"alpha\": best_alpha,\n",
    "            \"oof_preds\": oof_final,\n",
    "            \"test_preds\": test_final,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # 6) S√©lection finale\n",
    "    best_key = min(results.keys(), key=lambda k: results[k][\"oof_post\"])\n",
    "    print(f\"‚úÖ S√©lection finale: {best_key} ‚Äî OOF={results[best_key]['oof_post']:.6f} (alpha={results[best_key]['alpha']})\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"variant_scores\": scores,\n",
    "        \"blends\": results,\n",
    "        \"best\": best_key,\n",
    "        \"y_columns\": y_fit.columns.tolist(),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4873d",
   "metadata": {},
   "source": [
    "## üèÅ 6. Ex√©cution de la pipeline & g√©n√©ration de la soumission\n",
    "On ex√©cute le pipeline complet (variantes ‚Üí blend ‚Üí calibration ‚Üí shrink), on s√©lectionne le meilleur\n",
    "blend, puis on g√©n√®re le fichier de soumission final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ex√©cute la pipeline (CV=3 pour la d√©mo ; mettre 5 pour le run final)\n",
    "out = run_step1_diversity_blend(cv_splits=3, seed=42, do_per_target_weights=True)\n",
    "\n",
    "# R√©cup√®re la cl√© du meilleur blend et ses pr√©dictions test\n",
    "best_key = out[\"best\"]\n",
    "y_pred_blend = out[\"blends\"][best_key][\"test_preds\"]  # (n_test, n_targets_sans_c15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ccbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Colonnes cibles : on retire 'ID' et 'c15' du fit\n",
    "all_targets = [c for c in y_train.columns if c != \"ID\"]\n",
    "cols_fit = [c for c in all_targets if c != \"c15\"]\n",
    "\n",
    "# Cr√©e le DataFrame de pr√©dictions (clip pour rester dans [0,1])\n",
    "pred_df = pd.DataFrame(np.clip(y_pred_blend, 0, 1), columns=cols_fit)\n",
    "\n",
    "# R√©ins√®re la cible manquante 'c15'\n",
    "pred_df.insert(all_targets.index(\"c15\"), \"c15\", 0.0)\n",
    "\n",
    "# Assemble la soumission finale avec ID et ordre exact des colonnes\n",
    "submission = pd.concat([X_test[\"ID\"], pred_df[all_targets]], axis=1)\n",
    "submission.to_csv(\"submission_ET_BLEND_v1.csv\", index=False)\n",
    "\n",
    "\n",
    "# Diagnostics rapides\n",
    "vals = submission.drop(columns=[\"ID\"]).to_numpy()\n",
    "assert np.isfinite(vals).all(), \"NaN/Inf d√©tect√©s dans la soumission\"\n",
    "assert (vals >= -1e-9).all() and (vals <= 1+1e-9).all(), \"Valeurs hors [0,1]\"\n",
    "assert submission.index.equals(X_test.index)\n",
    "print(\"‚úÖ Submission pr√™te :\", submission.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9168d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
